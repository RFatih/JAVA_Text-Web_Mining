package test;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import trainset.PunctuationAndDigits;
import trainset.StopWords;
import zemberek.tokenization.TurkishTokenizer;

public class TF {

	public List<String> calculateTf (String file,Set<String> freq) throws IOException
	{
		List<String> rValue = new ArrayList<String>();
		TurkishTokenizer tokenizer = TurkishTokenizer.DEFAULT;
		TokenReader reader= new TokenReader();
		List<String> sWords= reader.read("C:\\Users\\fatih\\OneDrive\\Masa端st端\\metin-madenciligi\\stop-words", " ");
		//System.out.println(sWords);
	    List<String> tokens = tokenizer.tokenizeToStrings(file);
	    PunctuationAndDigits pd = new PunctuationAndDigits();
	    tokens=pd.removePD(tokens);
	    StopWords words= new StopWords();
	    tokens= words.removeStopWords(tokens, sWords);
	   // System.out.println(tokens);
	    Set<String> txtFreq = new HashSet<>(tokens);
	    //System.out.println(txtFreq.size());
	    List<String> allWords= reader.read("C:\\Users\\fatih\\OneDrive\\Masa端st端\\metin-madenciligi\\AllWords", "");
	    Double tf=0.0;
	    
	   
	    for (String word : freq)
	    {
	    	String max = findMax(word,txtFreq,tokens);
	    	tf = (double) (Collections.frequency(tokens, word) /(double)Collections.frequency(tokens, max));
	    	if(Collections.frequency(tokens, word)!=0)
	    	System.out.println(Collections.frequency(tokens, word) + " " +Collections.frequency(tokens, max) + " " + tf );
	    	rValue.add(tf.toString());
	    	
	    }
	    
	    
		return rValue;
	}
	public String findMax(String myWord,Set<String> freq,List<String> text)
	{
		String ans="";
		int maks=0;
		 for (String word : freq)
		    {
		    	if(maks<Collections.frequency(text, word) && word!=myWord && word!="")
		    	{
		    		maks = Collections.frequency(text, word);
		    		ans=word;
		    		//System.out.println(word);
		    	}
		    }

		//System.out.println(ans + maks);
		return ans;
	}
}
